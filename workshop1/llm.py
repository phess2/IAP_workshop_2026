import re
from typing import TypeVar

from openai import OpenAI
from pydantic import BaseModel, Field, field_validator

from .config import settings

T = TypeVar("T", bound=BaseModel)


def _parse_json_response(raw: str, model: type[T]) -> T:
    """Strip markdown fences and parse JSON into a Pydantic model."""
    cleaned = raw.strip()
    # Remove ```json or ``` fences
    cleaned = re.sub(r"^```(?:json)?\s*", "", cleaned)
    cleaned = re.sub(r"\s*```$", "", cleaned)
    return model.model_validate_json(cleaned)


MASTODON_MAX_CHARS = 475  # Reserve 25 chars for "\n\nPost generated by AI."


class MastodonPostContent(BaseModel):
    """Validated Mastodon post content."""

    content: str = Field(
        ...,
        description="The post content for Mastodon, max 475 characters",
    )
    hashtags: list[str] = Field(
        default_factory=list,
        description="Relevant hashtags (without # prefix)",
    )

    @field_validator("content")
    @classmethod
    def validate_content_length(cls, v: str) -> str:
        """Ensure content doesn't exceed Mastodon's character limit."""
        v = v.strip()
        if len(v) > MASTODON_MAX_CHARS:
            # Truncate intelligently at word boundary if possible
            truncated = v[: MASTODON_MAX_CHARS - 3]
            last_space = truncated.rfind(" ")
            if last_space > MASTODON_MAX_CHARS - 50:
                truncated = truncated[:last_space]
            return truncated + "..."
        return v

    def to_post_text(self) -> str:
        """Convert to final post text with hashtags appended."""
        text = self.content
        if self.hashtags:
            hashtag_str = " ".join(f"#{tag.lstrip('#')}" for tag in self.hashtags)
            combined = f"{text}\n\n{hashtag_str}"
            # Ensure combined doesn't exceed limit
            if len(combined) <= MASTODON_MAX_CHARS:
                return combined
        return text


class MastodonReplyContent(BaseModel):
    """Validated Mastodon reply content."""

    content: str = Field(
        ...,
        description="The reply content for Mastodon, max 475 characters",
    )

    @field_validator("content")
    @classmethod
    def validate_content_length(cls, v: str) -> str:
        """Ensure content doesn't exceed Mastodon's character limit."""
        v = v.strip()
        if len(v) > MASTODON_MAX_CHARS:
            truncated = v[: MASTODON_MAX_CHARS - 3]
            last_space = truncated.rfind(" ")
            if last_space > MASTODON_MAX_CHARS - 50:
                truncated = truncated[:last_space]
            return truncated + "..."
        return v


class LLMReplyResponse(BaseModel):
    """LLM-generated response for a single post in a batch."""

    response_text: str = Field(
        ...,
        description="The reply content for Mastodon, max 475 characters",
    )
    is_company_related: bool = Field(
        ...,
        description="Whether mentioning the company/business would be appropriate",
    )
    relevance_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="How relevant our expertise is to this post (0.0-1.0)",
    )
    reasoning: str = Field(
        ...,
        description="Brief explanation of why this response was crafted this way",
    )

    @field_validator("response_text")
    @classmethod
    def validate_response_length(cls, v: str) -> str:
        """Ensure response doesn't exceed Mastodon's character limit."""
        v = v.strip()
        if len(v) > MASTODON_MAX_CHARS:
            truncated = v[: MASTODON_MAX_CHARS - 3]
            last_space = truncated.rfind(" ")
            if last_space > MASTODON_MAX_CHARS - 50:
                truncated = truncated[:last_space]
            return truncated + "..."
        return v


class LLMReplyBatch(BaseModel):
    """Batch of LLM responses for multiple posts."""

    responses: list[LLMReplyResponse] = Field(
        ...,
        description="List of responses, one per input post in the same order",
    )


def _get_client() -> OpenAI:
    """Get OpenAI client configured for OpenRouter."""
    return OpenAI(
        api_key=settings.openrouter_api_key,
        base_url=settings.openrouter_base_url,
    )


def generate_post(business_context: str, page_content: str, page_title: str) -> str:
    """
    Generate a Mastodon post based on Notion page content.

    Args:
        business_context: The business description from the parent Notion page
        page_content: The content of the updated child page
        page_title: The title of the updated child page

    Returns:
        Generated post text (max 500 chars), validated by Pydantic
    """
    client = _get_client()

    system_prompt = """You are a social media manager creating Mastodon posts.
        Your posts should be:
        - Engaging and authentic
        - Concise (the main content should be under 400 characters to leave room for hashtags)
        - Written in a natural, human voice
        - Based on the business context and page content provided

        Do NOT use excessive emojis or sound overly promotional. Be genuine.

        You MUST respond with valid JSON in this exact format:
        {
        "content": "Your post text here (under 400 chars)",
        "hashtags": ["tag1", "tag2", "tag3"]
        }

        Provide 1-3 relevant hashtags without the # prefix."""

    user_prompt = f"""Business Description:
        {business_context}

        ---

        Page Title: {page_title}

        Page Content:
        {page_content}

        ---

        Write a Mastodon post for the described account, based on the page title and content. Respond with JSON only."""

    response = client.responses.create(
        model=settings.openrouter_model,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    post = _parse_json_response(response.output_text, MastodonPostContent)
    return post.to_post_text()


def generate_reply(
    post_content: str,
    post_author: str,
    business_context: str,
) -> str:
    """
    Generate a human, supportive reply to a Mastodon post.

    Args:
        post_content: The content of the post to reply to
        post_author: The author's display name or handle
        business_context: Business description for tone/context (not for promotion)

    Returns:
        Generated reply text, validated by Pydantic
    """
    client = _get_client()

    system_prompt = """You are writing a reply to a Mastodon post. Your reply should be:
        - Human, warm, and genuine
        - Supportive and connective
        - Brief (1-3 sentences typically)
        - NOT promotional at all - do not mention your business or products
        - Engaging with what the person actually said
        - Natural conversation, like a real person would respond

        You're building community and genuine connections, not selling anything.
        Never use corporate speak. Be a real person having a real conversation.
        Avoid emojis unless the original post clearly uses them.
        Do not include hashtags.

        You MUST respond with valid JSON in this exact format:
        {
        "content": "Your reply text here"
        }

        Just the reply text, nothing else."""

    user_prompt = f"""Your business context (for understanding your perspective, NOT for promotion):
        {business_context}

        ---

        Post by {post_author}:
        "{post_content}"

        ---

        Write a friendly, genuine reply."""

    response = client.responses.create(
        model=settings.openrouter_model,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    parsed = _parse_json_response(response.output_text, MastodonReplyContent)
    return parsed.content


def generate_replies_batch(
    posts: list[dict],
    business_context: str,
) -> list[LLMReplyResponse]:
    """
    Generate replies for multiple posts in a single batch LLM call.

    Uses JSON output parsing to get consistent, validated responses for all posts
    in one API call, which is more efficient than multiple individual calls.

    Args:
        posts: List of dicts with 'content' and 'author' keys
        business_context: Business description for tone/context (not for promotion)

    Returns:
        List of LLMReplyResponse objects in the same order as input posts
    """
    if not posts:
        return []

    client = _get_client()

    # Format all posts for the batch request
    posts_text = "\n\n".join(
        f'Post {i + 1} by {p["author"]}:\n"{p["content"]}"' for i, p in enumerate(posts)
    )

    system_prompt = f"""You are a community engagement specialist writing replies to Mastodon posts.

Your business context (for understanding your perspective, NOT for promotion):
{business_context}

Your replies should be:
- Human, warm, and genuine
- Supportive and connective
- Brief (1-3 sentences typically)
- NOT promotional - do not mention your business or products unless truly relevant
- Engaging with what the person actually said
- Natural conversation, like a real person would respond

You're building community and genuine connections, not selling anything.
Never use corporate speak. Be a real person having a real conversation.
Avoid emojis unless the original post clearly uses them.
Do not include hashtags.

For each post, determine:
- Whether mentioning your company/business would be appropriate (is_company_related)
- A relevance score (0.0-1.0) for how relevant your expertise is to this topic
- Brief reasoning for your approach
- The actual response text (max 475 characters)

You MUST respond with valid JSON in this exact format:
{{
  "responses": [
    {{
      "response_text": "Your reply text here (max 475 chars)",
      "is_company_related": true or false,
      "relevance_score": 0.0 to 1.0,
      "reasoning": "Brief explanation"
    }}
  ]
}}

Return one response object per post, in the same order as the posts."""

    user_prompt = f"""Generate responses for these {len(posts)} Mastodon posts. Return exactly {len(posts)} response(s) in order. Respond with JSON only.

Posts to respond to:
{posts_text}"""

    response = client.chat.completions.create(
        model=settings.openrouter_model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    parsed = _parse_json_response(response.choices[0].message.content, LLMReplyBatch)
    return parsed.responses


class ImagePromptContent(BaseModel):
    """Validated image prompt content."""

    prompt: str = Field(
        ...,
        description="The image prompt starting with 'OCTGUY octopus'",
    )


def generate_image_prompt(post_content: str) -> str:
    """
    Generate an image prompt for a diffusion model based on post content.

    Args:
        post_content: The generated Mastodon post content

    Returns:
        Image prompt string starting with "OCTGUY octopus"
    """
    client = _get_client()

    system_prompt = """You are creating image prompts for a diffusion model.
        Convert the post content into a creative, visual description that will generate
        an engaging image related to the post's theme.

        The prompt MUST start with "OCTGUY octopus" and then describe a creative,
        visually interesting scene related to the post content.

        You MUST respond with valid JSON in this exact format:
        {
        "prompt": "OCTGUY octopus <creative visual description>"
        }

        Make the description vivid and specific, focusing on visual elements that
        would make an interesting image."""

    user_prompt = f"""Post content:
        {post_content}

        ---

        Create an image prompt starting with "OCTGUY octopus" that relates to this post.
        Respond with JSON only."""

    response = client.responses.create(
        model=settings.openrouter_model,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    parsed = _parse_json_response(response.output_text, ImagePromptContent)

    # Ensure prompt starts with "OCTGUY octopus" (fallback if LLM doesn't follow instructions)
    prompt = parsed.prompt.strip()
    if not prompt.lower().startswith("octguy octopus"):
        prompt = f"OCTGUY octopus {prompt}"

    return prompt
